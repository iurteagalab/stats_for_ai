{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: the GP prior model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a GP model with [GPytorch](https://gpytorch.ai/)\n",
    "\n",
    "The next cell demonstrates the most critical features of a user-defined Gaussian process model in GPyTorch.\n",
    "\n",
    "- In contrast to many existing GP packages, GPyTorch does not provide full GP models for the user.\n",
    "- Rather, it provides *the tools necessary to quickly construct one*.\n",
    "- This is because we believe, analogous to building a neural network in standard PyTorch, it is important to have the flexibility to include whatever components are necessary: this allows the user great flexibility in designing custom models.\n",
    "\n",
    "For most GP regression models, you will need to construct the following GPyTorch objects:\n",
    "\n",
    "1. A **GP Model** (`gpytorch.models.ExactGP`).\n",
    "1. A **Mean** - This defines the prior mean of the GP.\n",
    "    - (If you don't know which mean to use, a `gpytorch.means.ConstantMean()` is a good place to start.\n",
    "1. A **Kernel** - This defines the prior covariance of the GP.\n",
    "    - If you don't know which kernel to use, a `gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())` is a good place to start.\n",
    "  \n",
    "### The GP Model\n",
    "  \n",
    "The components of a user built (Exact) GP model in GPyTorch are, broadly speaking:\n",
    "\n",
    "1. An `__init__` method that takes the data and a likelihood, and constructs whatever objects are necessary for the model's `forward` method. This will most commonly include things like a mean module and a kernel module.\n",
    "\n",
    "2. A `forward` method that takes in some $n \\times d$ data `x` and returns a `MultivariateNormal` with the *prior* mean and covariance evaluated at `x`.\n",
    "    - It returns a **MultivariateNormal** Distribution (`gpytorch.distributions.MultivariateNormal`), the object used to represent multivariate normal distributions. \n",
    "    - In other words, it returns the vector $\\mu(x)$ and the $n \\times n$ matrix $K_{xx}$ representing the prior mean and covariance matrix of the GP.\n",
    "    \n",
    "This specification leaves a large amount of flexibility when defining a model. For example, to compose two kernels via addition, you can either add the kernel modules directly:\n",
    "\n",
    "```python\n",
    "self.covar_module = ScaleKernel(RBFKernel() + LinearKernel())\n",
    "```\n",
    "\n",
    "Or you can add the outputs of the kernel in the forward method:\n",
    "\n",
    "```python\n",
    "covar_x = self.rbf_kernel_module(x) + self.white_noise_module(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model modes\n",
    "\n",
    "Like most PyTorch modules, the `ExactGP` has a `.train()` and `.eval()` mode.\n",
    "- `.eval()` mode is for computing predictions through the model posterior.\n",
    "- `.train()` mode is for optimizing model hyperameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact GP model definition\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, x, y, mean_f, kernel_f, likelihood_f):\n",
    "        super(ExactGPModel, self).__init__(x, y, likelihood_f)\n",
    "        self.mean_module = mean_f\n",
    "        self.covar_module = kernel_f\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiations of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data, to later be replaced with real data, now used for plotting the prior GP model\n",
    "x = torch.linspace(-1, 1, 100)\n",
    "y = torch.ones(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean function\n",
    "mean_f = gpytorch.means.ConstantMean()\n",
    "# Kernel function\n",
    "kernel_f = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "# Likelihood function\n",
    "likelihood_f = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# GP model:\n",
    "gp_model = ExactGPModel(\n",
    "    x,\n",
    "    y,\n",
    "    mean_f,\n",
    "    kernel_f,\n",
    "    likelihood_f\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the prior\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "\n",
    "    # The mean and covariance of the prior\n",
    "    gp_model_prior = gp_model(x)\n",
    "\n",
    "    # Plot the mean\n",
    "    plt.plot(x.numpy(), gp_model_prior.mean.numpy(), 'k')\n",
    "    # Plot the 95% confidence interval\n",
    "    lower, upper = gp_model_prior.confidence_region()\n",
    "    plt.fill_between(x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    plt.title('Prior GP model')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(['Mean', 'Confidence interval'])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples from the prior\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "    for _ in range(5):\n",
    "        sample = gp_model_prior.sample()\n",
    "        # Plot the samples, changing the color each sample\n",
    "        # to make it easier to see the samples\n",
    "        plt.plot(x.numpy(), sample.numpy(),\n",
    "                 color=plt.cm.viridis(torch.rand(1).item()), alpha=0.5)\n",
    "    plt.title('Samples from the prior GP model')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid()\n",
    "    plt.legend(['Samples $f(x)\\sim GP$'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prior GP model, along with some samples\n",
    "# from the prior GP model\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "    for _ in range(5):\n",
    "        sample = gp_model_prior.sample()\n",
    "        # Plot the samples, changing the color each sample\n",
    "        # to make it easier to see the samples\n",
    "        plt.plot(x.numpy(), sample.numpy(),\n",
    "                 color=plt.cm.viridis(torch.rand(1).item()), alpha=0.5)\n",
    "    # Plot the mean\n",
    "    plt.plot(x.numpy(), gp_model_prior.mean.numpy(), 'k')\n",
    "    # Plot the 95% confidence interval\n",
    "    lower, upper = gp_model_prior.confidence_region()\n",
    "    plt.fill_between(x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    plt.title('Samples and the prior GP model')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid()\n",
    "    plt.legend(['Samples $f(x)\\sim GP$'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate the above plot for different GP priors\n",
    "\n",
    "Check out GPytorch documentation:\n",
    "\n",
    "- [Different mean functions](https://docs.gpytorch.ai/en/stable/means.html)\n",
    "\n",
    "- [Different kernel functions](https://docs.gpytorch.ai/en/stable/kernels.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean functions\n",
    "# Constant mean function\n",
    "\n",
    "# Linear mean function with bias\n",
    "\n",
    "# Kernel functions\n",
    "# RBF kernel\n",
    "\n",
    "# Mattern kernel\n",
    "\n",
    "# Periodic kernel\n",
    "\n",
    "# Linear kernel\n",
    "\n",
    "# Polynomial kernel\n",
    "\n",
    "# Cosine kernel\n",
    "\n",
    "# Likelihood function\n",
    "likelihood_f = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prior GP model, along with some samples, from each prior GP model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP prior to posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe some data\n",
    "x = torch.linspace(-1, 1, 50)\n",
    "x_full = torch.linspace(-5, 5, 500)\n",
    "# Observation noise\n",
    "sigma = 0.1\n",
    "y_linear = 2 * x + 1 + torch.randn(x.size()) * sigma\n",
    "\n",
    "# Plot the data\n",
    "plt.figure()\n",
    "plt.scatter(x.numpy(), y_linear.numpy(), label='Linear data')\n",
    "plt.title('Observed data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a GP prior model\n",
    "\n",
    "# Mean function\n",
    "mean_f = gpytorch.means.ConstantMean()\n",
    "# Kernel function\n",
    "kernel_f = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "# Likelihood function\n",
    "likelihood_f = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP model with observed data:\n",
    "gp_model_linear = ExactGPModel(\n",
    "    x,\n",
    "    y_linear,\n",
    "    mean_f,\n",
    "    kernel_f,\n",
    "    likelihood_f\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prior GP model\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "\n",
    "    # The mean and covariance of the prior, over full x\n",
    "    gp_model_prior = gp_model_linear(x)\n",
    "\n",
    "    # Plot the mean\n",
    "    plt.plot(x.numpy(), gp_model_prior.mean.numpy(), 'k')\n",
    "    # Plot the 95% confidence interval\n",
    "    lower, upper = gp_model_prior.confidence_region()\n",
    "    plt.fill_between(x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    plt.title('Prior GP model with linear data')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(['Mean', 'Confidence interval'])\n",
    "    plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the posterior GP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote a test point (`test_x`) as `x*` with the true output being `y*`, then `model(test_x)` returns the model posterior distribution `p(f* | x*, X, y)`, for training data `X, y`.\n",
    "    - This posterior is the distribution over the function we are trying to model, and thus quantifies our model uncertainty.\n",
    "\n",
    "```python\n",
    "f_preds = model(test_x)\n",
    "\n",
    "f_mean = f_preds.mean\n",
    "f_var = f_preds.variance\n",
    "f_covar = f_preds.covariance_matrix\n",
    "f_samples = f_preds.sample(sample_shape=torch.Size(1000,))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the posterior GP model along with some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP model with observed data:\n",
    "gp_model_linear = ExactGPModel(\n",
    "    x,\n",
    "    y_linear,\n",
    "    mean_f,\n",
    "    kernel_f,\n",
    "    likelihood_f\n",
    ")\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "gp_model_linear.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "\n",
    "    # The mean and covariance of the posterior\n",
    "    gp_model_posterior = gp_model_linear(x_full)\n",
    "\n",
    "    # Plot the mean\n",
    "    plt.plot(x_full.numpy(), gp_model_posterior.mean.numpy(), 'k')\n",
    "    # Plot the 95% confidence interval\n",
    "    lower, upper = gp_model_posterior.confidence_region()\n",
    "    plt.fill_between(x_full.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    # Plot some samples from the posterior\n",
    "    for _ in range(5):\n",
    "        sample = gp_model_posterior.sample()\n",
    "        # Plot the samples, changing the color each sample\n",
    "        # to make it easier to see the samples\n",
    "        plt.plot(x_full.numpy(), sample.numpy(),\n",
    "                 color=plt.cm.viridis(torch.rand(1).item()), alpha=0.5)\n",
    "    # Plot the observations\n",
    "    plt.scatter(x.numpy(), y_linear.numpy(), color='red', marker='x')\n",
    "    plt.title('Posterior GP model and samples with linear data')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(['Mean', 'Confidence interval', 'Samples $f(x)\\sim GP$'])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with the model\n",
    "\n",
    "In the next cell, we make predictions with the model. To do this, we simply put the model and likelihood in eval mode, and call both modules on the test data.\n",
    "\n",
    "If we denote a test point (`test_x`) as `x*` with the true output being `y*`, then `model(test_x)` returns the model posterior distribution `p(f* | x*, X, y)`, for training data `X, y`.\n",
    "    - This posterior is the distribution over the function we are trying to model, and thus quantifies our model uncertainty.\n",
    "\n",
    "In contrast, `likelihood(model(test_x))` gives us the posterior predictive distribution `p(y* | x*, X, y)` which is the probability distribution over the predicted output value.\n",
    "    - By including the _likelihood noise_ which is the noise in your observation (e.g. due to noisy sensor), the prediction is over the observed value of the test point.\n",
    "\n",
    "Thus, getting the predictive mean and variance, and then sampling functions from the GP at the given test points could be accomplished with calls like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions of the GP model with linear data, along with some samples, incorporating the likelihood\n",
    "gp_model_linear = ExactGPModel(\n",
    "    x,\n",
    "    y_linear,\n",
    "    mean_f,\n",
    "    kernel_f,\n",
    "    likelihood_f\n",
    ")\n",
    "# Set the model in evaluation mode\n",
    "gp_model_linear.eval()\n",
    "# Set the likelihood in evaluation mode\n",
    "likelihood_f.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "\n",
    "    # The mean and covariance of the predictions, over region of interest\n",
    "    x_interest = torch.linspace(-2, 2, 400)\n",
    "    gp_model_prediction = likelihood_f(gp_model_linear(x_interest))\n",
    "    # Plot the mean\n",
    "    plt.plot(x_interest.numpy(), gp_model_prediction.mean.numpy(), 'k')\n",
    "    # Plot the 95% confidence interval\n",
    "    lower, upper = gp_model_prediction.confidence_region()\n",
    "    plt.fill_between(x_interest.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    # Plot some predicted samples\n",
    "    for _ in range(5):\n",
    "        sample = gp_model_prediction.sample()\n",
    "        # Plot the samples, changing the color each sample\n",
    "        # to make it easier to see the samples\n",
    "        plt.plot(x_interest.numpy(), sample.numpy(),\n",
    "                 color=plt.cm.viridis(torch.rand(1).item()), alpha=0.5)\n",
    "    # Plot the observations\n",
    "    plt.scatter(x.numpy(), y_linear.numpy(), color='red', marker='x')\n",
    "    plt.title('GP model predictions and samples with linear data')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(['Mean', 'Confidence interval', 'Samples $f(x)\\sim GP$'])\n",
    "    plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A GP model\n",
    "\n",
    "In the next cell, we handle using Type-II MLE to train the hyperparameters of the Gaussian process.\n",
    "\n",
    "In GPyTorch, we make use of the standard PyTorch optimizers as from `torch.optim`, and all trainable parameters of the model should be of type `torch.nn.Parameter`.\n",
    "    - Because GP models directly extend `torch.nn.Module`, calls to methods like `model.parameters()` or `model.named_parameters()` function as you might expect coming from PyTorch.\n",
    "\n",
    "In most cases, the boilerplate code below will work well. It has the same basic components as the standard PyTorch training loop:\n",
    "\n",
    "1. Zero all parameter gradients\n",
    "2. Call the model and compute the loss\n",
    "3. Call backward on the loss to fill in gradients\n",
    "4. Take a step on the optimizer\n",
    "\n",
    "However, defining custom training loops allows for greater flexibility.\n",
    "    - For example, it is easy to save the parameters at each step of training, or use different learning rates for different parameters ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_gp_model(model, likelihood, train_x, train_y, training_iter=50):\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1) \n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # Iterate in training\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            model.likelihood.noise.item()\n",
    "        ))\n",
    "        optimizer.step()\n",
    "\n",
    "    # Return the model and the likelihood\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a GP prior\n",
    "gp_model_linear = ExactGPModel(\n",
    "    x,\n",
    "    y_linear,\n",
    "    mean_f,\n",
    "    kernel_f,\n",
    "    likelihood_f\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prior GP model\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "\n",
    "    # The mean and covariance of the prior\n",
    "    gp_model_prior = gp_model_linear(x)\n",
    "\n",
    "    # Plot the mean\n",
    "    plt.plot(x.numpy(), gp_model_prior.mean.numpy(), 'k')\n",
    "    # Plot the 95% confidence interval\n",
    "    lower, upper = gp_model_prior.confidence_region()\n",
    "    plt.fill_between(x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    plt.title('Prior GP model with linear data')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(['Mean', 'Confidence interval'])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior GP model with linear data\n",
    "gp_model_linear.eval()\n",
    "likelihood_f.eval()\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "\n",
    "    # The mean and covariance of the posterior\n",
    "    gp_model_posterior = gp_model_linear(x_full)\n",
    "\n",
    "    # Plot the mean\n",
    "    plt.plot(x_full.numpy(), gp_model_posterior.mean.numpy(), 'k')\n",
    "    # Plot the 95% confidence interval\n",
    "    lower, upper = gp_model_posterior.confidence_region()\n",
    "    plt.fill_between(x_full.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    # Plot some samples from the posterior\n",
    "    for _ in range(5):\n",
    "        sample = gp_model_posterior.sample()\n",
    "        # Plot the samples, changing the color each sample\n",
    "        # to make it easier to see the samples\n",
    "        plt.plot(x_full.numpy(), sample.numpy(),\n",
    "                 color=plt.cm.viridis(torch.rand(1).item()), alpha=0.5)\n",
    "    # Plot the observations\n",
    "    plt.scatter(x.numpy(), y_linear.numpy(), color='red', marker='x')\n",
    "    plt.title('Posterior GP model and samples with linear data')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(['Mean', 'Confidence interval', 'Samples $f(x)\\sim GP$'])\n",
    "    plt.grid()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model and likelihood in training mode\n",
    "gp_model_linear.train()\n",
    "likelihood_f.train()\n",
    "\n",
    "# Train the GP model\n",
    "gp_model_linear_trained, likelihood_f_trained = train_gp_model(\n",
    "    gp_model_linear,\n",
    "    likelihood_f,\n",
    "    x,\n",
    "    y_linear,\n",
    "    training_iter=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prior, trained GP model\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "\n",
    "    # The mean and covariance of the prior\n",
    "    gp_model_prior = gp_model_linear_trained(x)\n",
    "\n",
    "    # Plot the mean\n",
    "    plt.plot(x.numpy(), gp_model_prior.mean.numpy(), 'k')\n",
    "    # Plot the 95% confidence interval\n",
    "    lower, upper = gp_model_prior.confidence_region()\n",
    "    plt.fill_between(x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    plt.title('Prior trained GP model with linear data')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(['Mean', 'Confidence interval'])\n",
    "    plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior, trained GP model with linear data\n",
    "gp_model_linear_trained.eval()\n",
    "likelihood_f_trained.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "\n",
    "    # The mean and covariance of the posterior\n",
    "    gp_model_posterior = gp_model_linear_trained(x_full)\n",
    "\n",
    "    # Plot the mean\n",
    "    plt.plot(x_full.numpy(), gp_model_posterior.mean.numpy(), 'k')\n",
    "    # Plot the 95% confidence interval\n",
    "    lower, upper = gp_model_posterior.confidence_region()\n",
    "    plt.fill_between(x_full.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    # Plot some samples from the posterior\n",
    "    for _ in range(5):\n",
    "        sample = gp_model_posterior.sample()\n",
    "        # Plot the samples, changing the color each sample\n",
    "        # to make it easier to see the samples\n",
    "        plt.plot(x_full.numpy(), sample.numpy(),\n",
    "                 color=plt.cm.viridis(torch.rand(1).item()), alpha=0.5)\n",
    "    # Plot the observations\n",
    "    plt.scatter(x.numpy(), y_linear.numpy(), color='red', marker='x')\n",
    "    plt.title('Posterior trained GP model and samples with linear data')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(['Mean', 'Confidence interval', 'Samples $f(x)\\sim GP$'])\n",
    "    plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions of the trained GP model with linear data: i.e., incorporate the likelihood\n",
    "gp_model_linear_trained.eval()\n",
    "likelihood_f_trained.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "\n",
    "    # The mean and covariance of the predictions\n",
    "    gp_model_prediction = likelihood_f_trained(gp_model_linear_trained(x_full))\n",
    "    # Plot the mean\n",
    "    plt.plot(x_full.numpy(), gp_model_prediction.mean.numpy(), 'k')\n",
    "    # Plot the 95% confidence interval\n",
    "    lower, upper = gp_model_prediction.confidence_region()\n",
    "    plt.fill_between(x_full.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    # Plot some predicted samples\n",
    "    for _ in range(5):\n",
    "        sample = gp_model_prediction.sample()\n",
    "        # Plot the samples, changing the color each sample\n",
    "        # to make it easier to see the samples\n",
    "        plt.plot(x_full.numpy(), sample.numpy(),\n",
    "                 color=plt.cm.viridis(torch.rand(1).item()), alpha=0.5)\n",
    "    # Plot the observations\n",
    "    plt.scatter(x.numpy(), y_linear.numpy(), color='red', marker='x')\n",
    "    plt.title('Trained GP model predictions and samples with linear data')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(['Mean', 'Confidence interval', 'Samples $f(x)\\sim GP$'])\n",
    "    plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate the above with different data and GP models\n",
    "\n",
    "- How does the prior mean and covariance function influence the prior and the posterior?\n",
    "\n",
    "- How does the posterior change close and far from data?\n",
    "\n",
    "- How does the prior change before and after training?\n",
    "\n",
    "- How does the posterior change before and after training?\n",
    "\n",
    "- What happens with different datasets?\n",
    "\n",
    "- How does the likelihood function affect predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "2025_stats_for_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
