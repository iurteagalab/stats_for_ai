{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Useful sklearn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for logistic regression\n",
    "def real_data_logistic():\n",
    "    # We will load the Titanic dataset form openml\n",
    "    try:\n",
    "        titanic_data = fetch_openml(\n",
    "            name=\"titanic\",\n",
    "            version=1,\n",
    "            as_frame=True\n",
    "        )\n",
    "        df_titanic = titanic_data.frame.drop('body', axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Titanic dataset: {e}.\")\n",
    "\n",
    "    # Features\n",
    "    X_titanic = df_titanic.drop('survived', axis=1)\n",
    "    # Target variable\n",
    "    y_titanic = df_titanic['survived']\n",
    "\n",
    "    return X_titanic, y_titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get real data for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = real_data_logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data, visualizing it, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of data do we have?\n",
    "print('X data types:')\n",
    "print(X.dtypes)\n",
    "print('\\n')\n",
    "print('y data types:')\n",
    "print(y.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to dummies\n",
    "X = pd.get_dummies(\n",
    "    X,\n",
    "    columns=['sex','embarked'],\n",
    "    drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's encode the categorical variables\n",
    "def encode_categorical(X):\n",
    "    # Select the categorical columns\n",
    "    cat_cols = X.select_dtypes(include=['object']).columns\n",
    "    print(f'Categorical columns: {cat_cols}')\n",
    "    \n",
    "    # Create a label encoder object\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Apply the label encoder to each column\n",
    "    for col in cat_cols:\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "# The rest, simply encode\n",
    "X_encoded = encode_categorical(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values\n",
    "X_features = X_encoded.dropna()\n",
    "y_target = y.loc[X_features.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets, using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Logistic Regression model\n",
    "model_logistic = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    "  )\n",
    "\n",
    "# MLE fitting\n",
    "model_logistic.fit(\n",
    "    X = X_train,\n",
    "    y = y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the coefficients\n",
    "print('Model coefficients:')\n",
    "print(model_logistic.coef_)\n",
    "print('Model intercept:')\n",
    "print(model_logistic.intercept_)\n",
    "print('Model classes:')\n",
    "print(model_logistic.classes_)\n",
    "print('Model number of iterations:')\n",
    "print(model_logistic.n_iter_)\n",
    "print('Model score:')\n",
    "print(model_logistic.score(X_features, y_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training set\n",
    "y_pred_train = model_logistic.predict(X_train)\n",
    "# Predict on the test set\n",
    "y_pred_test = model_logistic.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"--- Logistic Regression (Train Set) ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_train, y_pred_train))\n",
    "print(\"\\n--- Logistic Regression (Test Set) ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond accuracy: ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and AUC, using sklearn\n",
    "# Import necessary libraries\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# Predict probabilities\n",
    "# Predict probabilities for the test set\n",
    "# Note: The predict_proba method returns the probabilities for each class\n",
    "# and we want the probability of the positive class (1)\n",
    "y_prob = model_logistic.predict_proba(X_test)[:, 1]\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(\n",
    "    y_test,\n",
    "    y_prob,\n",
    "    pos_label='1'\n",
    ")\n",
    "# Calculate the AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, c='blue', label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve using sklearn's RocCurveDisplay\n",
    "RocCurveDisplay.from_estimator(\n",
    "    model_logistic,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    name='Logistic Regression',\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How does the model perform on the training set?\n",
    "# Predict probabilities for the training set\n",
    "y_prob_train = model_logistic.predict_proba(X_train)[:, 1]\n",
    "# Calculate the ROC curve for the training set\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(\n",
    "    y_train,\n",
    "    y_prob_train,\n",
    "    pos_label='1'\n",
    ")\n",
    "# Calculate the AUC for the training set\n",
    "roc_auc_train = roc_auc_score(y_train, y_prob_train)\n",
    "print(\"ROC AUC (Train Set):\", roc_auc_train)\n",
    "# Plot the ROC curve using sklearn's RocCurveDisplay for the training set\n",
    "RocCurveDisplay.from_estimator(\n",
    "    model_logistic,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    name='Logistic Regression (Train Set)',\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for both training and test sets\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, c='blue', label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot(fpr_train, tpr_train, c='orange', label='ROC curve (Train Set area = {:.2f})'.format(roc_auc_train))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# Calculate precision and recall\n",
    "# Predict probabilities for the training set\n",
    "y_prob_train = model_logistic.predict_proba(X_train)[:, 1]\n",
    "# Calculate precision and recall for the training set\n",
    "precision_train, recall_train, thresholds_pr_train = precision_recall_curve(\n",
    "    y_train,\n",
    "    y_prob_train,\n",
    "    pos_label='1'\n",
    ")\n",
    "# Calculate the average precision score for the training set\n",
    "avg_precision_train = average_precision_score(y_train, y_prob_train, pos_label='1')\n",
    "print(\"Average Precision (Train Set):\", avg_precision_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_prob = model_logistic.predict_proba(X_test)[:, 1]\n",
    "# Calculate precision and recall for the test set\n",
    "precision, recall, thresholds_pr = precision_recall_curve(\n",
    "    y_test,\n",
    "    y_prob,\n",
    "    pos_label='1'\n",
    ")\n",
    "# Calculate the average precision score for the test set\n",
    "avg_precision = average_precision_score(y_test, y_prob, pos_label='1')\n",
    "print(\"Average Precision (Test Set):\", avg_precision)\n",
    "\n",
    "# Plot the precision-recall curve, for both training and test sets\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, c='blue', label='Precision-Recall curve (area = {:.2f})'.format(avg_precision))\n",
    "plt.plot(recall_train, precision_train, c='orange', label='Precision-Recall curve (Train Set area = {:.2f})'.format(avg_precision_train))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the precision-recall curve using sklearn's PrecisionRecallDisplay\n",
    "PrecisionRecallDisplay.from_estimator(\n",
    "    model_logistic,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    name='Logistic Regression',\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does all of the above change with\n",
    "\n",
    "- Number of features?\n",
    "\n",
    "- Number of samples?\n",
    "\n",
    "- Different train-test splits?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025_stats_for_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
