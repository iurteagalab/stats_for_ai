{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data: A linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Number of samples\n",
    "n = 100\n",
    "\n",
    "# Explanatory variable or features or covariates\n",
    "# Uniformly drawn from [0, 1]\n",
    "X = np.random.uniform(0, 1, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data-generating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear function, with parameters a and b\n",
    "# Gaussian noise with standard deviation sigma\n",
    "def true_function(a,b,X,sigma=1):\n",
    "    # Target variable with Gaussian noise\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate realizations of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear true parameters\n",
    "a=4\n",
    "b=2\n",
    "sigma=1.0\n",
    "\n",
    "# Generate the target variable\n",
    "y = true_function(a,b,X,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE solution\n",
    "\n",
    "Estimate the linear regression parameters, after observing some data\n",
    "\n",
    "The assumed model is\n",
    "\n",
    "$$y = \\beta^\\top x_b + \\epsilon, \\qquad \\text{ with } \\qquad x_b= (1, x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the best fit line using the MLE solution:\n",
    "\n",
    "$$\\beta^* = (X_b^\\top X_b)^-1 X_b^\\top y$$\n",
    "\n",
    "where $X_b$ is the design matrix (with the bias terms added), $X_b^T$ is the transpose of $X_b$, and $y$ is the vector of observed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add x0 = 1 to each instance\n",
    "X_b = np.c_[np.ones((n)), X]\n",
    "\n",
    "# Compute the MLE solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute estimation errors, using MSE as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In prediction: $y_{\\text{pred}} = X_b \\beta^*$\n",
    "y_pred = \n",
    "y_mse = np.mean((y_pred - y)**2)\n",
    "print(\"MSE in prediction: \", y_mse)\n",
    "\n",
    "# In parameter estimation, MSE\n",
    "a_est = \n",
    "b_est = \n",
    "a_mse = np.mean((a_est - a)**2)\n",
    "b_mse = np.mean((b_est - b)**2)\n",
    "print(\"MSE in estimating a: \", a_mse)\n",
    "print(\"MSE in estimating b: \", b_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make and plot predictions of $y$ in the $x \\in [-1, 2]$ range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[-1], [2]])\n",
    "X_new_b = np.c_[np.ones((2)), X_new]  # Add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(beta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the best fit line\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([-1, 2, min(y)-2, max(y)+2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, replicate the above analysis:\n",
    "\n",
    "- Removing a single data point from an existing dataset at each experiment\n",
    "\n",
    "- Compute the MLE line for each experiment\n",
    "\n",
    "- Plot the MLE lines on the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples for this experiment\n",
    "n_samples = 25\n",
    "\n",
    "# Feature (independent variable)\n",
    "# # Uniformly drawn from [0, 1]\n",
    "X = np.random.uniform(0, 1, n_samples)\n",
    "y = true_function(a, b, X, sigma=1.0)\n",
    "\n",
    "# Number of experiments\n",
    "n_exp = 10\n",
    "\n",
    "# Initialize arrays to store the learned parameters\n",
    "beta_best = np.zeros((2, n_exp))\n",
    "\n",
    "# Initialize arrays to store the errors\n",
    "y_mse = np.zeros(n_exp)\n",
    "a_mse = np.zeros(n_exp)\n",
    "b_mse = np.zeros(n_exp)\n",
    "\n",
    "# Iterate over the number of experiments\n",
    "for i in range(n_exp):\n",
    "    # Select a random data point to ignore\n",
    "    idx = np.random.randint(0, n_samples)\n",
    "\n",
    "    # Remove the selected data point\n",
    "    X_new = np.delete(X, idx)\n",
    "    y_new = np.delete(y, idx)\n",
    "\n",
    "    # Compute the MLE solution\n",
    "    \n",
    "\n",
    "    # Compute the errors\n",
    "    y_pred = X_new_b.dot(beta_best[..., i])\n",
    "    y_mse[i] = np.mean((y_pred - y_new)**2)\n",
    "    a_mse[i] = np.mean((beta_best[1, i] - a)**2)\n",
    "    b_mse[i] = np.mean((beta_best[0, i] - b)**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learned linear functions\n",
    "X_new = np.array([[-1], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # Add x0 = 1 to each\n",
    "\n",
    "# Plot the learned linear functions\n",
    "for i in range(n_exp):\n",
    "    y_predict = \n",
    "    plt.plot(X_new, y_predict, label='Experiment {}'.format(i))\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([-1, 2, min(y)-2, max(y)+2])\n",
    "# Show the legend in the lower right corner\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Learned linear functions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate the above analysis for a range of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "n_samples= np.arange(10, 101, 10)\n",
    "\n",
    "# Initialize arrays to store \n",
    "# the learned parameters\n",
    "beta_best = np.zeros((2,len(n_samples)))\n",
    "# the errors, in MSE\n",
    "y_mse = np.zeros(len(n_samples))\n",
    "a_mse = np.zeros(len(n_samples))\n",
    "b_mse = np.zeros(len(n_samples))\n",
    "\n",
    "# Iterate over the number of samples\n",
    "for i,n_sample in enumerate(n_samples):\n",
    "    # Generate random features\n",
    "    \n",
    "    # Generate the target variable\n",
    "    \n",
    "    # MLE solution\n",
    "    \n",
    "    \n",
    "    # Compute the errors\n",
    "    y_pred = X_b.dot(beta_best[...,i])\n",
    "    y_mse[i] = np.mean((y_pred - y)**2)\n",
    "    a_mse[i] = np.mean((beta_best[1,i] - a)**2)\n",
    "    b_mse[i] = np.mean((beta_best[0,i] - b)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learned parameters and their estimation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the learned parameters a\n",
    "plt.hist(?, bins=10)\n",
    "plt.xlabel('Learned parameter a')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of learned parameter a')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of learned parameters b\n",
    "plt.hist(?, bins=10)\n",
    "plt.xlabel('Learned parameter b')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of learned parameter b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the error in parameter estimation, as a function of the number of samples\n",
    "plt.plot(n_samples, a_mse, label='Error in estimating a')\n",
    "plt.plot(n_samples, b_mse, label='Error in estimating b')\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE in parameter estimation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the error in prediction, as a function of the number of samples\n",
    "plt.plot(n_samples, y_mse)\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE in prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the above plot, what value is the MSE hovering around?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learned linear functions, for a range of number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting rante\n",
    "X_new = np.array([[-1], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # Add x0 = 1 to each instance\n",
    "\n",
    "# Plot the learned linear functions\n",
    "for i in range(len(n_samples)):\n",
    "    y_predict = ?\n",
    "    plt.plot(X_new, y_predict, label='n_samples = {}'.format(n_samples[i]))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([-1, 2, min(y)-2, max(y)+2])\n",
    "# Show the legend in the lower right corner\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Learned linear functions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a prior distribution for beta\n",
    "-  Use Gaussian priors for simplicity:\n",
    "    - $y = \\beta^\\top x_b + \\epsilon$\n",
    "    - $p(\\beta) \\sim N(\\mu_0, \\Sigma_0)$ with mean $\\mu_0$ and covariance matrix $\\Sigma_0$ \n",
    "        -  Prior parameters represent our initial beliefs about the beta values before seeing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider different priors for beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior mean for beta\n",
    "prior_mean = np.array([0, 0])\n",
    "\n",
    "# Prior covariance for beta: l2 regularization type prior\n",
    "prior_cov = np.array([[1, 0], [0, 1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "n = 100\n",
    "\n",
    "# Explanatory variable or features or covariates\n",
    "X = np.random.uniform(-1, 2, n)\n",
    "\n",
    "# Define the data-generating function\n",
    "a = 4\n",
    "b = 2\n",
    "sigma = 1.0\n",
    "y = true_function(a, b, X, sigma)\n",
    "\n",
    "# Define X_b\n",
    "X_b = np.c_[np.ones((n)), X]  # Add x0 = 1 to each instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For a Gaussian likelihood, the posterior distribution is also Gaussian\n",
    "\n",
    "the posterior distribution is a Gaussian distribution, with mean and covariance given by:\n",
    "\n",
    "- posterior_cov: $\\Sigma_n = (\\frac{X_b^T * X_b}{\\sigma^2} + \\Sigma_0^{-1})^{-1}$\n",
    "- posterior_mean: $\\mu_n = \\Sigma_n * (X_b^T * y / \\sigma^2 + \\Sigma_0^{-1} * \\mu_0)$\n",
    "\n",
    "where we must know observation noise $\\sigma$, which is assumed to be known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the posterior covariance\n",
    "posterior_cov = ?\n",
    "# Compute the posterior mean\n",
    "posterior_mean = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the posterior distribution for beta: 1-D marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the marginal posterior distribution for beta_0\n",
    "# Range of beta_0 values to plot\n",
    "beta0 = np.linspace(\n",
    "    posterior_mean[0] - 3 * np.sqrt(posterior_cov[0, 0]),\n",
    "    posterior_mean[0] + 3 * np.sqrt(posterior_cov[0, 0]),\n",
    "    100\n",
    ")\n",
    "# Compute the marginal posterior distribution for beta_0\n",
    "f_beta0 = stats.norm.pdf(\n",
    "    beta0,\n",
    "    posterior_mean[0],\n",
    "    np.sqrt(posterior_cov[0, 0])\n",
    ")\n",
    "# Plot it\n",
    "plt.plot(beta0, f_beta0, label='beta_0')\n",
    "plt.xlabel('beta_0')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Marginal posterior distribution for beta_0')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the marginal posterior distribution for beta_1\n",
    "# Range of beta_1 values to plot\n",
    "beta1 = np.linspace(\n",
    "    posterior_mean[1] - 3 * np.sqrt(posterior_cov[1, 1]),\n",
    "    posterior_mean[1] + 3 * np.sqrt(posterior_cov[1, 1]),\n",
    "    100\n",
    ")\n",
    "# Compute the marginal posterior distribution for beta_1\n",
    "f_beta1 = stats.norm.pdf(\n",
    "    beta1,\n",
    "    posterior_mean[1],\n",
    "    np.sqrt(posterior_cov[1, 1])\n",
    ")\n",
    "# Plot it\n",
    "plt.plot(beta1, f_beta1, label='beta_1')\n",
    "plt.xlabel('beta_1')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Marginal posterior distribution for beta_1')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the posterior distribution for beta: the 2D joint distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of beta_0 and beta_1 values\n",
    "beta0, beta1 = np.meshgrid(\n",
    "    np.linspace(\n",
    "        posterior_mean[0] - 3 * np.sqrt(posterior_cov[0, 0]),\n",
    "        posterior_mean[0] + 3 * np.sqrt(posterior_cov[0, 0]),\n",
    "        100\n",
    "    ),\n",
    "    np.linspace(\n",
    "        posterior_mean[1] - 3 * np.sqrt(posterior_cov[1, 1]),\n",
    "        posterior_mean[1] + 3 * np.sqrt(posterior_cov[1, 1]),\n",
    "        100\n",
    "    )\n",
    ")\n",
    "# Compute the joint posterior distribution\n",
    "f_beta = stats.multivariate_normal.pdf(\n",
    "    np.dstack((beta0, beta1)),\n",
    "    mean=posterior_mean,\n",
    "    cov=posterior_cov\n",
    ")\n",
    "# Plot the joint posterior distribution\n",
    "plt.contourf(beta0, beta1, f_beta)\n",
    "plt.xlabel('beta_0')\n",
    "plt.ylabel('beta_1')\n",
    "plt.title('Joint posterior distribution for beta_0 and beta_1')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The posterior predictive distribution: marginalize uncertainty over parameters to predict!\n",
    "\n",
    "The predictive posterior is in general defined as:\n",
    "\n",
    "$$P(y_{new} | x_{new}, X, y) = \\int P(y_{new} | X_{new}, \\beta) P(\\beta | X, y) d\\beta$$\n",
    "\n",
    "For the case in hand, with Gaussian priors and a Gaussian likelihood, the posterior predictive distribution is also a Gaussian distribution:\n",
    "\n",
    "$$P(y_{new} | x_{new}, X, y) = \\ N(y_{new} | \\mu_{y_{new}}, \\Sigma_{y_{new}})$$\n",
    "\n",
    "with mean and covariance given by the following equations, where we must know observation noise $\\sigma^2$:\n",
    "\n",
    "- predictive_mean: $\\qquad \\mu_{y_{new}} = x_{new} * \\mu_n$\n",
    "- predictive_cov: $\\qquad \\Sigma_{y_{new}} = x_{new} * \\Sigma_n * x_{new}^T + \\sigma^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature range will now cover a wider range, e.g., [-5, 5]\n",
    "X_new = np.arange(-5, 5, 0.01)\n",
    "X_new_b = np.c_[np.ones(len(X_new)), X_new]  # Add x0 = 1 to each instance\n",
    "\n",
    "# Compute the predictive mean\n",
    "predictive_mean = ?\n",
    "# Compute the predictive covariance\n",
    "predictive_cov = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the posterior predictive distribution, which we know is Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean of the posterior predictive distribution\n",
    "plt.plot(\n",
    "    X_new,\n",
    "    predictive_mean,\n",
    "    label='Mean of the predictive distribution'\n",
    ")\n",
    "# Plot the 95% credible interval of the posterior predictive distribution\n",
    "plt.fill_between(\n",
    "    X_new,\n",
    "    predictive_mean - 1.96 * np.sqrt(np.diag(predictive_cov)),\n",
    "    predictive_mean + 1.96 * np.sqrt(np.diag(predictive_cov)),\n",
    "    alpha=0.2,\n",
    "    label='95% credible interval'\n",
    ")\n",
    "plt.plot(X, y, 'bo', label='Data')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Posterior predictive distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that, for any given x_new, the predictive distribution over $y$ is Gaussian:\n",
    "\n",
    "$$y \\ | \\ x_{new}, X, y \\sim \\mathcal{N}(\\mu_{y}(x_{new}), \\sigma^2_{y}(x_{new}))$$\n",
    "\n",
    "with mean and variance given by the following expressions:\n",
    "\n",
    "- $\\mu_{y}(x_{new}) = x_{new}^T \\mu_n$\n",
    "- $\\sigma^2_{y}(x_{new}) = x_{new}^T \\Sigma_n x_{new} + \\sigma^2$\n",
    "\n",
    "where $\\mu_n$ and $\\Sigma_n$ are the posterior mean and covariance, respectively, and $\\sigma^2$ is the noise variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior predictive distribution, which we know is Gaussian, for a given x_new\n",
    "x_new = 1\n",
    "x_new_b = np.array([1, x_new])\n",
    "# Compute the predictive mean, at x_new\n",
    "predictive_mean_xnew = x_new_b.dot(posterior_mean)\n",
    "# Compute the predictive covariance, at x_new\n",
    "predictive_cov_xnew = x_new_b @ (posterior_cov) @ x_new_b.T + sigma**2\n",
    "# Compute the predictive distribution, at x_new, which is Gaussian with mean and covariance\n",
    "predictive_dist_xnew = stats.norm(predictive_mean_xnew, np.sqrt(predictive_cov_xnew))\n",
    "# Plot the predictive distribution, over a range of y values\n",
    "y_new = np.linspace(predictive_mean_xnew - 3 * np.sqrt(predictive_cov_xnew), predictive_mean_xnew + 3 * np.sqrt(predictive_cov_xnew), 100)\n",
    "plt.plot(\n",
    "    y_new,\n",
    "    predictive_dist_xnew.pdf(y_new),\n",
    "    label='Predictive distribution'\n",
    ")\n",
    "plt.xlabel('Predicted y value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predictive distribution over y at x_new = {}'.format(x_new))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the uncertainty in the prediction vary according to whether we have regions where we have more or less data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the uncertainty in the prediction, over a wide x range\n",
    "plt.plot(X_new, np.sqrt(np.diag(predictive_cov)))\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Uncertainty in the prediction')\n",
    "plt.title('Uncertainty in the prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What value is lower-bounding the uncertainty in the posterior predictive distribution?\n",
    "\n",
    "-  The uncertainty in the posterior predictive distribution is lower-bounded by the noise level sigma.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the error in prediction\n",
    "\n",
    "-  The error in prediction is given by the mean squared error (MSE), between the predicted target values and the true target values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predicted target values\n",
    "y_pred = \n",
    "# Compute the error in prediction\n",
    "y_mse = np.mean((y_pred - y)**2)\n",
    "print('MSE in prediction:', y_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate the above prediction error analysis for a range of number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "n_samples = np.arange(10, 101, 10)\n",
    "n_samples = np.arange(100, 10001, 100)\n",
    "\n",
    "# Initialize arrays to store the learned parameters\n",
    "beta_best = np.zeros((2, len(n_samples)))\n",
    "# Initialize arrays to store the errors\n",
    "y_mse = np.zeros(len(n_samples))\n",
    "a_mse = np.zeros(len(n_samples))\n",
    "b_mse = np.zeros(len(n_samples))\n",
    "\n",
    "# Define prior distributions for beta\n",
    "# Prior mean for beta\n",
    "prior_mean = np.array([0, 0])\n",
    "# Prior covariance for beta\n",
    "prior_cov = np.array([[1, 0], [0, 1]])\n",
    "prior_cov = np.array([[10, 0], [0, 10]])\n",
    "\n",
    "# Iterate over the number of samples\n",
    "for i, n_sample in enumerate(n_samples):\n",
    "    # Generate random features\n",
    "    X = np.random.uniform(-1, 2, n_sample)\n",
    "    # Generate the target variable\n",
    "    y = true_function(a, b, X, sigma=1.0)\n",
    "    X_b = np.c_[np.ones((n_sample)), X]  # Add x0 = 1 to each instance\n",
    "\n",
    "    # Compute the posterior distribution for beta\n",
    "    posterior_cov = ?\n",
    "    posterior_mean = ?\n",
    "\n",
    "    # Compute the errors\n",
    "    y_pred = X_b.dot(posterior_mean)\n",
    "    y_mse[i] = np.mean((y_pred - y)**2)\n",
    "    a_mse[i] = np.mean((posterior_mean[1] - a)**2)\n",
    "    b_mse[i] = np.mean((posterior_mean[0] - b)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the error in parameter estimation, as a function of the number of samples\n",
    "plt.plot(n_samples, a_mse, label='Error in estimating a')\n",
    "plt.plot(n_samples, b_mse, label='Error in estimating b')\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE in parameter estimation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction error, as a function of the number of samples\n",
    "plt.plot(n_samples, y_mse)\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE in prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What value does the MSE in prediction converge to as the number of samples increases?\n",
    "\n",
    "-  The MSE in prediction converges to the observation noise sigma squared as the number of samples increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you feel brave enough to do a full Bayesian analysis? \n",
    "\n",
    "- Then, use a Gaussian prior for the parameters a and b, and an inverse Gamma prior for the noise variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025_stats_for_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
