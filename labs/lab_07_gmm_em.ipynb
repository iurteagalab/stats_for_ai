{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM-related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the likelihood of a GMM\n",
    "def gmm_lik(x, pi, mu, sigma):\n",
    "    \"\"\"Calculates the probability density of a GMM.\"\"\"\n",
    "    k = len(pi)\n",
    "    density = np.zeros_like(x)\n",
    "    for i in range(k):\n",
    "        density += ???\n",
    "    return density\n",
    "\n",
    "# Function to compute the log-likelihood of a GMM\n",
    "def gmm_log_lik(x, pi, mu, sigma):\n",
    "    \"\"\"Calculates the log-likelihood of a GMM.\"\"\"\n",
    "    likelihood = gmm_lik(x, pi, mu, sigma)\n",
    "    return np.sum(np.log(likelihood + 1e-10))  # Adding a small constant to avoid log(0)\n",
    "\n",
    "# Function to compute the posterior probabilities of the GMM\n",
    "def gmm_posterior_probabilities(x, pi, mu, sigma):\n",
    "    \"\"\"Calculates the posterior probabilities of a GMM.\"\"\"\n",
    "    k = len(pi)\n",
    "    likelihood = gmm_lik(x, pi, mu, sigma)\n",
    "    posteriors = np.zeros((len(x), k))\n",
    "    for i in range(k):\n",
    "        posteriors[:, i] = pi[i] * multivariate_normal.pdf(x, mu[i], sigma[i])\n",
    "    posteriors /= likelihood[:, np.newaxis]\n",
    "    return posteriors\n",
    "\n",
    "# Function to sample from a GMM\n",
    "def gmm_sample(pi, mu, sigma, n_samples):\n",
    "    \"\"\"Samples from a GMM.\"\"\"\n",
    "    k = len(pi)\n",
    "    z = ??\n",
    "    x = ??\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM inference algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the k-means algorithm (i.e., the same as above but with hard assignments) for GMM\n",
    "def kmeans_gmm(x, k, max_iters=100, tol=1e-6):\n",
    "    \"\"\"Performs k-means algorithm for GMM.\"\"\"\n",
    "    n, d = x.shape\n",
    "    pi = np.ones(k) / k\n",
    "    mu = x[np.random.choice(n, k, replace=False)]\n",
    "    sigma = [np.eye(d) for _ in range(k)]\n",
    "\n",
    "    # Initialize lists to store log-likelihoods and ELBOs\n",
    "    log_likelihoods = []\n",
    "\n",
    "    # Iterate until max iterations or convergence, given tolerance\n",
    "    for iteration in range(max_iters):\n",
    "        # Responsibility step\n",
    "        responsibilities = np.zeros((n, k))\n",
    "        ???\n",
    "        \n",
    "        # Assign each point to the cluster with the highest responsibility\n",
    "        z_n = np.argmax(responsibilities, axis=1)\n",
    "\n",
    "        # Update the parameters\n",
    "        nk = np.bincount(z_n, minlength=k)\n",
    "        pi = nk / n\n",
    "        mu = np.array([x[z_n == i].mean(axis=0) for i in range(k)])\n",
    "        for i in range(k):\n",
    "            diff = x[z_n == i] - mu[i]\n",
    "            sigma[i] = np.dot(diff.T, diff) / nk[i]\n",
    "        \n",
    "        # Calculate Loglikelihood of assigned clusters and data, given the parameters.\n",
    "        log_likelihood = np.sum(np.log(np.sum([pi[j] * multivariate_normal.pdf(xi, mu[j], sigma[j]) for j in range(k)], axis=0)) for xi in x)\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # The current estimated GMM\n",
    "        plt.scatter(x[:, 0], x[:, 1], alpha=0.5)\n",
    "        for i in range(k):\n",
    "            # Plot the mean of each cluster\n",
    "            plt.scatter(mu[i, 0], mu[i, 1], marker='x', s=100, c='orange', label=f'Cluster {i+1} mean')\n",
    "            # Draw ellipse for covariance matrix for 2-D case\n",
    "            if d == 2:\n",
    "                from matplotlib.patches import Ellipse\n",
    "                eigenvalues, eigenvectors = np.linalg.eigh(sigma[i])\n",
    "                angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "                width, height = 2 * np.sqrt(eigenvalues)\n",
    "                ell = Ellipse(xy=mu[i], width=width, height=height, angle=angle, edgecolor='orange', fc='None', lw=2, label=f'Cluster {i+1} covariance')\n",
    "                plt.gca().add_patch(ell)\n",
    "        plt.title(f\"K-Means Iteration {iteration + 1} - Estimated GMM Parameters\")\n",
    "        plt.xlabel(\"Feature 1\")\n",
    "        plt.ylabel(\"Feature 2\")\n",
    "        plt.xlim(np.min(x[:, 0]), np.max(x[:, 0]))\n",
    "        plt.ylim(np.min(x[:, 1]), np.max(x[:, 1]))\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Check for convergence\n",
    "        if iteration > 0 and abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
    "            break\n",
    "    \n",
    "    # Plotting log-likelihood over iterations\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(log_likelihoods)\n",
    "    plt.title(\"K-means Log likelihood over iterations\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Log likelihood\")\n",
    "    plt.show()\n",
    "    \n",
    "    return pi, mu, sigma, log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the EM algorithm for GMM\n",
    "def em_gmm(x, k, max_iters=100, tol=1e-6):\n",
    "    \"\"\"Performs EM algorithm for GMM.\"\"\"\n",
    "    n, d = x.shape\n",
    "    pi = np.ones(k) / k\n",
    "    mu = x[np.random.choice(n, k, replace=False)]\n",
    "    sigma = [np.eye(d) for _ in range(k)]\n",
    "\n",
    "    # Initialize lists to store log-likelihoods and ELBOs\n",
    "    log_likelihoods = []\n",
    "    elbos = []\n",
    "    kl_divergences = []\n",
    "\n",
    "    # Iterate until max iterations or convergence, given tolerance\n",
    "    for iteration in range(max_iters):\n",
    "        # E-step\n",
    "        responsibilities = np.zeros((n, k))\n",
    "        ??\n",
    "\n",
    "        # M-step\n",
    "        nk = responsibilities.sum(axis=0)\n",
    "        pi = nk / n\n",
    "        mu = np.dot(responsibilities.T, x) / nk[:, np.newaxis]\n",
    "        for i in range(k):\n",
    "            diff = x - mu[i]\n",
    "            sigma[i] = np.dot(responsibilities[:, i] * diff.T, diff) / nk[i]\n",
    "\n",
    "        # Calculate ELBO\n",
    "        elbo = 0\n",
    "        for i in range(n):\n",
    "            for j in range(k):\n",
    "                if responsibilities[i,j] >0:\n",
    "                    elbo += responsibilities[i, j] * (np.log(pi[j]) + multivariate_normal.logpdf(x[i], mu[j], sigma[j]) - np.log(responsibilities[i,j]))\n",
    "        elbos.append(elbo)\n",
    "\n",
    "        #Calculate Loglikelihood.\n",
    "        log_likelihood = np.sum(np.log(np.sum([pi[j] * multivariate_normal.pdf(xi, mu[j], sigma[j]) for j in range(k)], axis=0)) for xi in x)\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "\n",
    "        kl_divergence = log_likelihood - elbo\n",
    "        kl_divergences.append(kl_divergence)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # The current estimated GMM\n",
    "        plt.scatter(x[:, 0], x[:, 1], alpha=0.5)\n",
    "        for i in range(k):\n",
    "            plt.scatter(mu[i, 0], mu[i, 1], marker='x', s=100, c='green', label=f'Cluster {i+1} mean')\n",
    "            # Draw ellipse for covariance matrix for 2-D case\n",
    "            if d == 2:\n",
    "                from matplotlib.patches import Ellipse\n",
    "                eigenvalues, eigenvectors = np.linalg.eigh(sigma[i])\n",
    "                angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "                width, height = 2 * np.sqrt(eigenvalues)\n",
    "                ell = Ellipse(xy=mu[i], width=width, height=height, angle=angle, edgecolor='green', fc='None', lw=2, label=f'Cluster {i+1} covariance')\n",
    "                plt.gca().add_patch(ell)\n",
    "        plt.title(f\"EM Iteration {iteration + 1} - Estimated GMM\")\n",
    "        plt.xlabel(\"Feature 1\")\n",
    "        plt.ylabel(\"Feature 2\")\n",
    "        plt.xlim(np.min(x[:, 0]), np.max(x[:, 0]))\n",
    "        plt.ylim(np.min(x[:, 1]), np.max(x[:, 1]))\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if iteration > 0 and abs(elbos[-1] - elbos[-2]) < tol:\n",
    "            break\n",
    "\n",
    "    # Plotting ELBO and log-likelihood over iterations\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(log_likelihoods, label='Log Likelihood')\n",
    "    plt.plot(elbos, label='ELBO')\n",
    "    plt.title(\"EM: Log Likelihood and ELBO over iterations\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting KL divergence\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(kl_divergences)\n",
    "    plt.title(\"EM: KL Divergence between ELBO and Log likelihood over iterations\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"KL Divergence\")\n",
    "    plt.show()\n",
    "\n",
    "    return pi, mu, sigma, log_likelihoods, elbos, kl_divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate GMM and draw from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility, if needed\n",
    "# Define true parameters\n",
    "true_pi = [0.3, 0.7]\n",
    "true_mu = [[1, 1], [-1, -1]]\n",
    "true_sigma = [[[1, 0], [0, 1]], [[0.5, 0], [0, 0.5]]]\n",
    "n_samples = 500\n",
    "k = 2  # Number of components\n",
    "\n",
    "# Generate synthetic data\n",
    "x = gmm_sample(true_pi, true_mu, true_sigma, n_samples)\n",
    "\n",
    "# Plot the generated data\n",
    "plt.scatter(x[:, 0], x[:, 1], alpha=0.5)\n",
    "plt.title(\"Generated Data from GMM\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run k-means algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the k-means algorithm\n",
    "kmeans_pi, kmeans_mu, kmeans_sigma, kmeans_log_likelihoods = kmeans_gmm(\n",
    "    x,\n",
    "    k,\n",
    "    max_iters=100,\n",
    "    tol=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you modify the k-means algorithm to plot the assigned clusters to each data point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the EM algorithm\n",
    "em_pi, em_mu, em_sigma, em_log_likelihoods, em_elbos, em_kl_divergences = em_gmm(\n",
    "    x,\n",
    "    k,\n",
    "    max_iters=100,\n",
    "    tol=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results for K-means and EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute parameter errors\n",
    "# For k-means\n",
    "kmeans_pi_error = np.abs(np.array(kmeans_pi) - np.array(true_pi))\n",
    "kmeans_mu_error = np.array(kmeans_mu) - np.array(true_mu)\n",
    "kmeans_sigma_error = np.array(kmeans_sigma) - np.array(true_sigma)\n",
    "# For EM\n",
    "em_pi_error = np.abs(np.array(em_pi) - np.array(true_pi))\n",
    "em_mu_error = np.array(em_mu) - np.array(true_mu)\n",
    "em_sigma_error = np.array(em_sigma) - np.array(true_sigma)\n",
    "\n",
    "# Compare the estimated parameters with the true parameters\n",
    "print(\"True Mixing Coefficients:\", true_pi)\n",
    "print(\"K-means Estimated Mixing Coefficients:\", kmeans_pi)\n",
    "print(\"\\t_K-means Mixing Coefficient Errors:\", kmeans_pi_error)\n",
    "print(\"EM Estimated Mixing Coefficients:\", em_pi)\n",
    "print(\"\\tEM Mixing Coefficient Errors:\", em_pi_error)\n",
    "\n",
    "print(\"True Means:\", true_mu)\n",
    "print(\"K-means Estimated Means:\", kmeans_mu)\n",
    "print(\"\\tK-means Mean Errors:\", kmeans_mu_error)\n",
    "print(\"EM Estimated Means:\", em_mu)\n",
    "print(\"\\tEM Mean Errors:\", em_mu_error)\n",
    "\n",
    "print(\"True Covariances:\", true_sigma)\n",
    "print(\"K-means Estimated Covariances:\", kmeans_sigma)\n",
    "print(\"\\tK-means Covariance Errors:\", kmeans_sigma_error)\n",
    "print(\"EM Estimated Covariances:\", em_sigma)\n",
    "print(\"\\tEM Covariance Errors:\", em_sigma_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the estimated parameters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x[:, 0], x[:, 1], alpha=0.5)\n",
    "for i in range(k):\n",
    "    # Plot estimated means\n",
    "    # K-means\n",
    "    plt.scatter(kmeans_mu[i][0], kmeans_mu[i][1], marker='x', s=100, c='orange')\n",
    "    # EM \n",
    "    plt.scatter(em_mu[i][0], em_mu[i][1], marker='x', s=100, c='green')\n",
    "    # Plot true means\n",
    "    plt.scatter(true_mu[i][0], true_mu[i][1], marker='x', s=100, c='red')\n",
    "\n",
    "    # Plot estimated covariance matrices\n",
    "    # Draw ellipse for covariance matrix for 2-D case\n",
    "    if len(true_mu[i]) == 2:\n",
    "        from matplotlib.patches import Ellipse\n",
    "        # For estimated covariance matrix\n",
    "        # K-means\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(kmeans_sigma[i])\n",
    "        angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "        width, height = 2 * np.sqrt(eigenvalues)\n",
    "        ell = Ellipse(xy=kmeans_mu[i], width=width, height=height, angle=angle, edgecolor='orange', fc='None', lw=2)\n",
    "        plt.gca().add_patch(ell)\n",
    "        # EM\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(em_sigma[i])\n",
    "        angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "        width, height = 2 * np.sqrt(eigenvalues)\n",
    "        ell = Ellipse(xy=em_mu[i], width=width, height=height, angle=angle, edgecolor='green', fc='None', lw=2)\n",
    "        plt.gca().add_patch(ell)\n",
    "        # For true covariance matrix\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(true_sigma[i])\n",
    "        angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "        width, height = 2 * np.sqrt(eigenvalues)\n",
    "        ell = Ellipse(xy=true_mu[i], width=width, height=height, angle=angle, edgecolor='red', fc='None', lw=2)\n",
    "        plt.gca().add_patch(ell)\n",
    "\n",
    "plt.title(\"Estimated GMM Parameters\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the log-likelihoods\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(kmeans_log_likelihoods, label='K-means Log Likelihood', color='orange')\n",
    "plt.plot(em_log_likelihoods, label='EM Log Likelihood', color='green')\n",
    "plt.title(\"Log Likelihood over iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log Likelihood\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the final log-likelihoods\n",
    "print(\"Final K-means Log Likelihood:\", kmeans_log_likelihoods[-1])\n",
    "print(\"Final EM Log Likelihood:\", em_log_likelihoods[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the above algorithms change \n",
    "\n",
    "- with different true GMM parameters?\n",
    "\n",
    "- with different initializations?\n",
    "\n",
    "- with different number of samples?\n",
    "\n",
    "- with different number of clusters?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025_stats_for_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
